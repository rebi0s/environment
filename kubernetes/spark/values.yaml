# Copyright Broadcom, Inc. All Rights Reserved.
# SPDX-License-Identifier: APACHE-2.0

## @param extraDeploy Array of extra objects to deploy with the release
##
extraDeploy:
#- apiVersion: secrets.hashicorp.com/v1beta1
#  kind: VaultStaticSecret
#  metadata:
#    name: vault-s3-secret
#  spec:
#    type: kv-v2
#    # Mount path of the secrets backend
#    mount: s3
#    # Path to the secret
#    path: rebi0shom
    # Where to store the secrets, end user will create the secret
#    destination:
#      create: true
#      name: s3-secret
- apiVersion: v1
  kind: ConfigMap
  metadata:
    name: cm-install-deps
  data:
    install-deps.sh: |
      #!/bin/bash

      export DEPENDENCIES="org.postgresql:postgresql:42.6.0\
      ,org.apache.iceberg:iceberg-bundled-guava:1.5.0\
      ,org.apache.iceberg:iceberg-core:1.5.0\
      ,org.apache.iceberg:iceberg-aws:1.5.0\
      ,org.apache.iceberg:iceberg-spark:1.5.0\
      ,org.apache.iceberg:iceberg-spark-runtime-3.5_2.12:1.5.0\
      ,org.apache.iceberg:iceberg-spark-extensions-3.5_2.12:1.5.0\
      ,org.apache.iceberg:iceberg-hive-runtime:1.5.0\
      ,org.apache.iceberg:iceberg-hive-metastore:1.5.0\
      ,org.slf4j:slf4j-simple:2.0.7"

      export AWS_SDK_VERSION=2.20.120
      export AWS_MAVEN_GROUP=software.amazon.awssdk

      export AWS_PACKAGES=(
        "bundle"
        "url-connection-client"
      )

      for pkg in "${AWS_PACKAGES[@]}"; do
        export DEPENDENCIES+=",$AWS_MAVEN_GROUP:$pkg:$AWS_SDK_VERSION"
      done

      # Set space as the delimiter
      IFS=','

      #Read the split words into an array based on space delimiter
      read -a deparr <<< "$DEPENDENCIES"

      for dep in "${deparr[@]}"; do
        mvn dependency:get -Dartifact=$dep
      done

      find $HOME/.m2/. -iname *.jar -exec cp -r '{}' /jars \;
- apiVersion: v1
  kind: ConfigMap
  metadata:
    name: cm-env
  data:
    # S3_URI: https://s3.nuvem.unicamp.br
    # S3_BUCKET: rebi0shom
    #AWS_REGION: main
    #POSTGRES_CONNECTION_STRING: jdbc:postgresql://iceberg-postgresql:5432/db_iceberg
    #POSTGRES_USER: icbergcat
    #POSTGRES_PASSWORD: hNXz35UBRcAC
    #S3_FULL_URL: s3://rebi0shom/
    #BIOS_CATALOG: bios
    #DEPENDENCIES: org.postgresql:postgresql:42.6.0,org.apache.iceberg:iceberg-bundled-guava:1.5.0,org.apache.iceberg:iceberg-core:1.5.0,org.apache.iceberg:iceberg-aws:1.5.0,org.apache.iceberg:iceberg-spark:1.5.0,org.apache.iceberg:iceberg-spark-runtime-3.4_2.13:1.5.0,org.apache.iceberg:iceberg-spark-extensions-3.4_2.13:1.5.0,org.apache.iceberg:iceberg-hive-runtime:1.5.0,org.apache.iceberg:iceberg-hive-metastore:1.5.0,org.slf4j:slf4j-simple:2.0.7
    #AWS_SDK_VERSION: "2.20.120"
    #AWS_MAVEN_GROUP: software.amazon.awssdk
    #IP: "127.0.0.1"
    #PORT: "10000"
    S3_URI: https://s3.amazonaws.com
    S3_BUCKET: rebios-kube-env/
    POSTGRES_CONNECTION_STRING: jdbc:postgresql://3.91.223.236:32038/db_iceberg
    POSTGRES_USER: role_iceberg
    POSTGRES_PASSWORD: hNXz35UBRcAC
    S3_FULL_URL: s3a://rebios-kube-env/
    BIOS_CATALOG: bios
    DEPENDENCIES: org.postgresql:postgresql:42.6.0,org.apache.iceberg:iceberg-bundled-guava:1.5.0,org.apache.iceberg:iceberg-core:1.5.0,org.apache.iceberg:iceberg-aws:1.5.0,org.apache.iceberg:iceberg-spark:1.5.0,org.apache.iceberg:iceberg-spark-runtime-3.4_2.13:1.5.0,org.apache.iceberg:iceberg-spark-extensions-3.4_2.13:1.5.0,org.apache.iceberg:iceberg-hive-runtime:1.5.0,org.apache.iceberg:iceberg-hive-metastore:1.5.0,org.slf4j:slf4j-simple:2.0.7
    AWS_SDK_VERSION: "2.20.120"
    AWS_MAVEN_GROUP: software.amazon.awssdk
    IP: "127.0.0.1"
    PORT: "10000"
- apiVersion: v1
  kind: ConfigMap
  metadata:
    name: cm-start-scripts
  data:
    start-pyspark.sh: |
      #!/bin/bash
      mkdir -p /tmp/spark-events
      pyspark --packages $DEPENDENCIES \
          --conf spark.sql.defaultCatalog=${BIOS_CATALOG} \
          --conf spark.sql.extensions=org.apache.iceberg.spark.extensions.IcebergSparkSessionExtensions \
          --conf spark.sql.catalog.bios=org.apache.iceberg.spark.SparkCatalog \
          --conf spark.sql.catalog.bios.catalog-impl=org.apache.iceberg.jdbc.JdbcCatalog \
          --conf spark.sql.catalog.bios.uri=${POSTGRES_CONNECTION_STRING}\
          --conf spark.sql.catalog.bios.jdbc.user=${POSTGRES_USER} \
          --conf spark.sql.catalog.bios.jdbc.password=${POSTGRES_PASSWORD} \
          --conf spark.sql.catalog.bios.warehouse=${S3_FULL_URL} \
          --conf spark.sql.catalog.bios.io-impl=org.apache.iceberg.aws.s3.S3FileIO \
          --conf spark.sql.catalog.bios.s3.endpoint=${S3_URI} \
          --conf spark.sql.catalog.spark_catalog=org.apache.iceberg.spark.SparkSessionCatalog \
          --conf spark.sql.catalogImplementation=in-memory \
          --conf spark.executor.memory=6g \
          --conf spark.driver.memory=4g \
          --conf spark.jars=/jars/* \
          --conf spark.driver.extraJavaOptions="-Divy.cache.dir=/tmp -Divy.home=/tmp" 
    start-spark-sql.sh: |
      #!/bin/bash
      mkdir -p /tmp/spark-events
      spark-sql --packages $DEPENDENCIES \
          --conf spark.sql.defaultCatalog=${BIOS_CATALOG} \
          --conf spark.sql.extensions=org.apache.iceberg.spark.extensions.IcebergSparkSessionExtensions \
          --conf spark.sql.catalog.bios=org.apache.iceberg.spark.SparkCatalog \
          --conf spark.sql.catalog.bios.catalog-impl=org.apache.iceberg.jdbc.JdbcCatalog \
          --conf spark.sql.catalog.bios.uri=${POSTGRES_CONNECTION_STRING}\
          --conf spark.sql.catalog.bios.jdbc.user=${POSTGRES_USER} \
          --conf spark.sql.catalog.bios.jdbc.password=${POSTGRES_PASSWORD} \
          --conf spark.sql.catalog.bios.warehouse=${S3_FULL_URL} \
          --conf spark.sql.catalog.bios.io-impl=org.apache.iceberg.aws.s3.S3FileIO \
          --conf spark.sql.catalog.bios.s3.endpoint=${S3_URI} \
          --conf spark.sql.catalog.spark_catalog=org.apache.iceberg.spark.SparkSessionCatalog \
          --conf spark.eventLog.enabled=true \
          --conf spark.eventLog.dir=/tmp/spark-events \
          --conf spark.history.fs.logDirectory=/tmp/spark-events \
          --conf spark.sql.catalogImplementation=in-memory \
          --conf spark.executor.memory=6g \
          --conf spark.driver.memory=4g \
          --conf spark.jars=/jars/* \
          --conf spark.driver.extraJavaOptions="-Divy.cache.dir=/tmp -Divy.home=/tmp" 

## @param initScripts Dictionary of init scripts. Evaluated as a template.
## Specify dictionary of scripts to be run at first boot
## Alternatively, you can put your scripts under the files/docker-entrypoint-initdb.d directory
## For example:
## initScripts:
##   my_init_script.sh: |
##      #!/bin/sh
##      echo "Do something."
##
initScripts:
  copia-jars: |
    #!/bin/bash
    cp -r /jars/* /opt/bitnami/spark/jars/
  start-spark-thrift.sh: |
    #!/bin/bash
    mkdir -p /tmp/spark-events
    start-thriftserver.sh --packages $DEPENDENCIES \
        --conf spark.sql.defaultCatalog=${BIOS_CATALOG} \
        --conf spark.sql.extensions=org.apache.iceberg.spark.extensions.IcebergSparkSessionExtensions \
        --conf spark.sql.catalog.bios=org.apache.iceberg.spark.SparkCatalog \
        --conf spark.sql.catalog.bios.catalog-impl=org.apache.iceberg.jdbc.JdbcCatalog \
        --conf spark.sql.catalog.bios.uri=${POSTGRES_CONNECTION_STRING}\
        --conf spark.sql.catalog.bios.jdbc.user=${POSTGRES_USER} \
        --conf spark.sql.catalog.bios.jdbc.password=${POSTGRES_PASSWORD} \
        --conf spark.sql.catalog.bios.warehouse=${S3_FULL_URL} \
        --conf spark.sql.catalog.bios.io-impl=org.apache.iceberg.aws.s3.S3FileIO \
        --conf spark.sql.catalog.bios.s3.endpoint=${S3_URI} \
        --conf spark.sql.catalog.spark_catalog=org.apache.iceberg.spark.SparkSessionCatalog \
        --conf spark.eventLog.enabled=true \
        --conf spark.eventLog.dir=/tmp/spark-events \
        --conf spark.history.fs.logDirectory=/tmp/spark-events \
        --conf spark.sql.catalogImplementation=in-memory \
        --conf spark.executor.memory=6g \
        --conf spark.driver.memory=4g \
        --conf spark.jars=/jars/* \
        --conf spark.driver.extraJavaOptions="-Divy.cache.dir=/tmp -Divy.home=/tmp" &

  # copy_jars.sh: |
  #   #!/bin/bash

  #   cp /jars/* /opt/bitnami/spark/jars/

## @param initScriptsCM ConfigMap with the init scripts. Evaluated as a template.
## Note: This will override initScripts
##
initScriptsCM: ""
## @param initScriptsSecret Secret containing `/docker-entrypoint-initdb.d` scripts to be executed at initialization time that contain sensitive data. Evaluated as a template.
##
initScriptsSecret: ""


## Spark master specific configuration
##
master:
  ## @param master.existingConfigmap The name of an existing ConfigMap with your custom configuration for master
  ##
  existingConfigmap: ""
  ## @param master.containerPorts.http Specify the port where the web interface will listen on the master over HTTP
  ## @param master.containerPorts.https Specify the port where the web interface will listen on the master over HTTPS
  ## @param master.containerPorts.cluster Specify the port where the master listens to communicate with workers
  ##
  containerPorts:
    http: 8080
    https: 8480
    cluster: 7077
  ## @param master.automountServiceAccountToken Mount Service Account token in pod
  ##
  automountServiceAccountToken: false
  ## @param master.hostAliases Deployment pod host aliases
  ## https://kubernetes.io/docs/concepts/services-networking/add-entries-to-pod-etc-hosts-with-host-aliases/
  ##
  hostAliases: []
  ## @param master.extraContainerPorts Specify the port where the running jobs inside the masters listens
  ## ref: https://kubernetes.io/docs/reference/generated/kubernetes-api/v1.20/#containerport-v1-core
  ## e.g:
  ## - name: myapp
  ##   containerPort: 8000
  ##   protocol: TCP
  ##
  extraContainerPorts: []
  ## @param master.daemonMemoryLimit Set the memory limit for the master daemon
  ##
  daemonMemoryLimit: ""
  ## @param master.configOptions Use a string to set the config options for in the form "-Dx=y"
  ##
  configOptions: ""
  ## @param master.extraEnvVars Extra environment variables to pass to the master container
  ## For example:
  ## extraEnvVars:
  ##  - name: SPARK_DAEMON_JAVA_OPTS
  ##    value: -Dx=y
  ##
  extraEnvVars: []
  ## @param master.extraEnvVarsCM Name of existing ConfigMap containing extra env vars for master nodes
  ##
  extraEnvVarsCM: "cm-env"
  ## @param master.extraEnvVarsSecret Name of existing Secret containing extra env vars for master nodes
  ##
  extraEnvVarsSecret: ""
  ## Kubernetes Pods Security Context
  ## https://kubernetes.io/docs/tasks/configure-pod-container/security-context/
  ## @param master.podSecurityContext.enabled Enable security context
  ## @param master.podSecurityContext.fsGroupChangePolicy Set filesystem group change policy
  ## @param master.podSecurityContext.sysctls Set kernel settings using the sysctl interface
  ## @param master.podSecurityContext.supplementalGroups Set filesystem extra groups
  ## @param master.podSecurityContext.fsGroup Set master pod's Security Context Group ID
  ##
  podSecurityContext:
    enabled: true
    fsGroupChangePolicy: Always
    sysctls: []
    supplementalGroups: []
    fsGroup: 1001
  ## Configure Container Security Context
  ## ref: https://kubernetes.io/docs/tasks/configure-pod-container/security-context/#set-the-security-context-for-a-container
  ## @param master.containerSecurityContext.enabled Enabled containers' Security Context
  ## @param master.containerSecurityContext.seLinuxOptions [object,nullable] Set SELinux options in container
  ## @param master.containerSecurityContext.runAsUser Set containers' Security Context runAsUser
  ## @param master.containerSecurityContext.runAsGroup Set containers' Security Context runAsGroup
  ## @param master.containerSecurityContext.runAsNonRoot Set container's Security Context runAsNonRoot
  ## @param master.containerSecurityContext.privileged Set container's Security Context privileged
  ## @param master.containerSecurityContext.readOnlyRootFilesystem Set container's Security Context readOnlyRootFilesystem
  ## @param master.containerSecurityContext.allowPrivilegeEscalation Set container's Security Context allowPrivilegeEscalation
  ## @param master.containerSecurityContext.capabilities.drop List of capabilities to be dropped
  ## @param master.containerSecurityContext.seccompProfile.type Set container's Security Context seccomp profile
  ##
  containerSecurityContext:
    enabled: false
    seLinuxOptions: {}
    runAsUser: 0
    runAsGroup: 0
    runAsNonRoot: false
    privileged: true
    readOnlyRootFilesystem: false
    allowPrivilegeEscalation: true
    capabilities:
      drop: ["ALL"]
    seccompProfile:
      type: "RuntimeDefault"
  ## @param master.command Override default container command (useful when using custom images)
  ##
  command: []
  ## @param master.args Override default container args (useful when using custom images)
  ##
  args: []
  ## @param master.podAnnotations Annotations for pods in StatefulSet
  ##
  podAnnotations: {}
  ## @param master.podLabels Extra labels for pods in StatefulSet
  ##
  podLabels: {}
  ## @param master.podAffinityPreset Spark master pod affinity preset. Ignored if `master.affinity` is set. Allowed values: `soft` or `hard`
  ## ref: https://kubernetes.io/docs/concepts/scheduling-eviction/assign-pod-node/#inter-pod-affinity-and-anti-affinity
  ##
  podAffinityPreset: ""
  ## @param master.podAntiAffinityPreset Spark master pod anti-affinity preset. Ignored if `master.affinity` is set. Allowed values: `soft` or `hard`
  ## ref: https://kubernetes.io/docs/concepts/scheduling-eviction/assign-pod-node/#inter-pod-affinity-and-anti-affinity
  ##
  podAntiAffinityPreset: soft
  ## Spark master node affinity preset
  ## ref: https://kubernetes.io/docs/concepts/scheduling-eviction/assign-pod-node/#node-affinity
  ##
  nodeAffinityPreset:
    ## @param master.nodeAffinityPreset.type Spark master node affinity preset type. Ignored if `master.affinity` is set. Allowed values: `soft` or `hard`
    ##
    type: ""
    ## @param master.nodeAffinityPreset.key Spark master node label key to match Ignored if `master.affinity` is set.
    ## E.g.
    ## key: "kubernetes.io/e2e-az-name"
    ##
    key: ""
    ## @param master.nodeAffinityPreset.values Spark master node label values to match. Ignored if `master.affinity` is set.
    ## E.g.
    ## values:
    ##   - e2e-az1
    ##   - e2e-az2
    ##
    values: []
  ## @param master.affinity Spark master affinity for pod assignment
  ## ref: https://kubernetes.io/docs/concepts/configuration/assign-pod-node/#affinity-and-anti-affinity
  ## Note: master.podAffinityPreset, master.podAntiAffinityPreset, and master.nodeAffinityPreset will be ignored when it's set
  ##
  affinity: {}
  ## @param master.nodeSelector Spark master node labels for pod assignment
  ## ref: https://kubernetes.io/docs/concepts/scheduling-eviction/assign-pod-node/
  ##
  nodeSelector: {}
  ## @param master.tolerations Spark master tolerations for pod assignment
  ## ref: https://kubernetes.io/docs/concepts/configuration/taint-and-toleration/
  ##
  tolerations: []
  ## @param master.updateStrategy.type Master statefulset strategy type.
  ## ref: https://kubernetes.io/docs/concepts/workloads/controllers/deployment/#strategy
  ## e.g:
  ## updateStrategy:
  ##  type: RollingUpdate
  ##  rollingUpdate:
  ##    maxSurge: 25%
  ##    maxUnavailable: 25%
  ##
  updateStrategy:
    type: RollingUpdate
  ## @param master.priorityClassName master pods' priorityClassName
  ##
  priorityClassName: ""
  ## @param master.topologySpreadConstraints Topology Spread Constraints for pod assignment spread across your cluster among failure-domains. Evaluated as a template
  ## Ref: https://kubernetes.io/docs/concepts/workloads/pods/pod-topology-spread-constraints/#spread-constraints-for-pods
  ##
  topologySpreadConstraints: []
  ## @param master.schedulerName Name of the k8s scheduler (other than default) for master pods
  ## ref: https://kubernetes.io/docs/tasks/administer-cluster/configure-multiple-schedulers/
  ##
  schedulerName: ""
  ## @param master.terminationGracePeriodSeconds Seconds Redmine pod needs to terminate gracefully
  ## ref: https://kubernetes.io/docs/concepts/workloads/pods/pod/#termination-of-pods
  ##
  terminationGracePeriodSeconds: ""
  ## @param master.lifecycleHooks for the master container(s) to automate configuration before or after startup
  ##
  lifecycleHooks: {}
  ## @param master.extraVolumes Optionally specify extra list of additional volumes for the master pod(s)
  ##
  extraVolumes:
  - name: install-deps
    configMap:
      name: cm-install-deps
  - name: start-scripts
    configMap:
      name: cm-start-scripts
  - name: jars
    emptyDir: {}
  - name: spark-apps
    hostPath:
      path: /data/spark/master/apps
  - name: spark-extra-jars
    hostPath:
      path: /data/spark/master/extra-jars
  ## @param worker.extraVolumeMounts Optionally specify extra list of additional volumeMounts for the master container(s)
  ##
  extraVolumeMounts:
  - name: jars
    mountPath: /jars
  - name: start-scripts
    mountPath: /scripts
  - name: spark-apps
    mountPath: /data/spark/master/apps
  - name: spark-extra-jars
    mountPath: /data/spark/master/extra-jars
  ## @param master.extraVolumeClaimTemplates Optionally specify extra list of volumesClaimTemplates for the master statefulset
  ##
  extraVolumeClaimTemplates: []
  ## Container resource requests and limits
  ## ref: https://kubernetes.io/docs/concepts/configuration/manage-compute-resources-container/
  ## We usually recommend not to specify default resources and to leave this as a conscious
  ## choice for the user. This also increases chances charts run on environments with little
  ## resources, such as Minikube. If you do want to specify resources, uncomment the following
  ## lines, adjust them as necessary, and remove the curly braces after 'resources:'.
  ## @param master.resourcesPreset Set container resources according to one common preset (allowed values: none, nano, micro, small, medium, large, xlarge, 2xlarge). This is ignored if master.resources is set (master.resources is recommended for production).
  ## More information: https://github.com/bitnami/charts/blob/main/bitnami/common/templates/_resources.tpl#L15
  ##
  resourcesPreset: "none"
  ## @param master.resources Set container requests and limits for different resources like CPU or memory (essential for production workloads)
  ## Example:
  ## resources:
  ##   requests:
  ##     cpu: 2
  ##     memory: 512Mi
  ##   limits:
  ##     cpu: 3
  ##     memory: 1024Mi
  ##
  resources:
    requests:
      cpu: 200m
      memory: 512Mi
    limits:
      cpu: 2000m
      memory: 8192Mi
  ## Configure extra options for liveness probe
  ## ref: https://kubernetes.io/docs/tasks/configure-pod-container/configure-liveness-readiness-probes/#configure-probes
  ## @param master.livenessProbe.enabled Enable livenessProbe
  ## @param master.livenessProbe.initialDelaySeconds Initial delay seconds for livenessProbe
  ## @param master.livenessProbe.periodSeconds Period seconds for livenessProbe
  ## @param master.livenessProbe.timeoutSeconds Timeout seconds for livenessProbe
  ## @param master.livenessProbe.failureThreshold Failure threshold for livenessProbe
  ## @param master.livenessProbe.successThreshold Success threshold for livenessProbe
  ##
  livenessProbe:
    enabled: true
    initialDelaySeconds: 180
    periodSeconds: 20
    timeoutSeconds: 5
    failureThreshold: 6
    successThreshold: 1
  ## Configure extra options for readiness probe
  ## ref: https://kubernetes.io/docs/tasks/configure-pod-container/configure-liveness-readiness-probes/#configure-probes
  ## @param master.readinessProbe.enabled Enable readinessProbe
  ## @param master.readinessProbe.initialDelaySeconds Initial delay seconds for readinessProbe
  ## @param master.readinessProbe.periodSeconds Period seconds for readinessProbe
  ## @param master.readinessProbe.timeoutSeconds Timeout seconds for readinessProbe
  ## @param master.readinessProbe.failureThreshold Failure threshold for readinessProbe
  ## @param master.readinessProbe.successThreshold Success threshold for readinessProbe
  ##
  readinessProbe:
    enabled: true
    initialDelaySeconds: 30
    periodSeconds: 10
    timeoutSeconds: 5
    failureThreshold: 6
    successThreshold: 1
  ## Configure extra options for startup probe
  ## @param master.startupProbe.enabled Enable startupProbe
  ## @param master.startupProbe.initialDelaySeconds Initial delay seconds for startupProbe
  ## @param master.startupProbe.periodSeconds Period seconds for startupProbe
  ## @param master.startupProbe.timeoutSeconds Timeout seconds for startupProbe
  ## @param master.startupProbe.failureThreshold Failure threshold for startupProbe
  ## @param master.startupProbe.successThreshold Success threshold for startupProbe
  ##
  startupProbe:
    enabled: false
    initialDelaySeconds: 30
    periodSeconds: 10
    timeoutSeconds: 5
    failureThreshold: 6
    successThreshold: 1
  ## @param master.customLivenessProbe Custom livenessProbe that overrides the default one
  ##
  customLivenessProbe: {}
  ## @param master.customReadinessProbe Custom readinessProbe that overrides the default one
  ##
  customReadinessProbe: {}
  ## @param master.customStartupProbe Custom startupProbe that overrides the default one
  ##
  customStartupProbe: {}
  ## Network Policies
  ## Ref: https://kubernetes.io/docs/concepts/services-networking/network-policies/
  ##
  networkPolicy:
    ## @param master.networkPolicy.enabled Specifies whether a NetworkPolicy should be created
    ##
    enabled: false
    ## @param master.networkPolicy.allowExternal Don't require client label for connections
    ## The Policy model to apply. When set to false, only pods with the correct
    ## client label will have network access to the ports the application is listening
    ## on. When true, the app will accept connections from any source
    ## (with the correct destination port).
    ##
    allowExternal: true
    ## @param master.networkPolicy.allowExternalEgress Allow the pod to access any range of port and all destinations.
    ##
    allowExternalEgress: true
    ## @param master.networkPolicy.extraIngress [array] Add extra ingress rules to the NetworkPolicy
    ## e.g:
    ## extraIngress:
    ##   - ports:
    ##       - port: 1234
    ##     from:
    ##       - podSelector:
    ##           - matchLabels:
    ##               - role: frontend
    ##       - podSelector:
    ##           - matchExpressions:
    ##               - key: role
    ##                 operator: In
    ##                 values:
    ##                   - frontend
    extraIngress:
    - ports:
      - port: 7077
        protocol: TCP
      from:
      - podSelector:
          matchLabels:
            app.kubernetes.io/component: master
            app.kubernetes.io/instance: spark
            app.kubernetes.io/name: spark
    ## @param master.networkPolicy.extraEgress [array] Add extra ingress rules to the NetworkPolicy
    ## e.g:
    ## extraEgress:
    ##   - ports:
    ##       - port: 1234
    ##     to:
    ##       - podSelector:
    ##           - matchLabels:
    ##               - role: frontend
    ##       - podSelector:
    ##           - matchExpressions:
    ##               - key: role
    ##                 operator: In
    ##                 values:
    ##                   - frontend
    ##
    extraEgress: []
    ## @param master.networkPolicy.ingressNSMatchLabels [object] Labels to match to allow traffic from other namespaces
    ## @param master.networkPolicy.ingressNSPodMatchLabels [object] Pod labels to match to allow traffic from other namespaces
    ##
    ingressNSMatchLabels: {}
    ingressNSPodMatchLabels: {}
  ## @param master.sidecars Add additional sidecar containers to the master pod(s)
  ## e.g:
  ## sidecars:
  ##   - name: your-image-name
  ##     image: your-image
  ##     imagePullPolicy: Always
  ##     ports:
  ##       - name: portname
  ##         containerPort: 1234
  ##
  sidecars: []
  ## @param master.initContainers Add initContainers to the master pods.
  ## Example:
  ## initContainers:
  ##   - name: your-image-name
  ##     image: your-image
  ##     imagePullPolicy: Always
  ##     ports:
  ##       - name: portname
  ##         containerPort: 1234
  ##
  initContainers:
  - name: download-dependencies
    image: maven
    command:
    - sh
    - -ec
    - |
      cp /scripts/install-deps.sh /opt/install-deps.sh
      chmod +x /opt/install-deps.sh
      /opt/install-deps.sh
    volumeMounts:
    - mountPath: /jars
      name: jars
    - mountPath: /scripts
      name: install-deps
      
  ## Pod Disruption Budget configuration
  ## ref: https://kubernetes.io/docs/tasks/run-application/configure-pdb
  ## @param master.pdb.create Enable/disable a Pod Disruption Budget creation
  ## @param master.pdb.minAvailable Minimum number/percentage of pods that should remain scheduled
  ## @param master.pdb.maxUnavailable Maximum number/percentage of pods that may be made unavailable. Defaults to `1` if both `master.pdb.minAvailable` and `master.pdb.maxUnavailable` are empty.
  ##
  pdb:
    create: true
    minAvailable: ""
    maxUnavailable: ""
## @section Spark worker parameters
##

## Spark worker specific configuration
##
worker:
  ## @param worker.existingConfigmap The name of an existing ConfigMap with your custom configuration for workers
  ##
  existingConfigmap: ""
  ## @param worker.containerPorts.http Specify the port where the web interface will listen on the worker over HTTP
  ## @param worker.containerPorts.https Specify the port where the web interface will listen on the worker over HTTPS
  ## @param worker.containerPorts.cluster Specify the port where the worker listens to communicate with workers
  ##
  containerPorts:
    http: 8080
    https: 8480
    cluster: ""
  ## @param worker.automountServiceAccountToken Mount Service Account token in pod
  ##
  automountServiceAccountToken: false
  ## @param worker.hostAliases Add deployment host aliases
  ## https://kubernetes.io/docs/concepts/services-networking/add-entries-to-pod-etc-hosts-with-host-aliases/
  ##
  hostAliases: []
  ## @param worker.extraContainerPorts Specify the port where the running jobs inside the workers listens
  ## ref: https://kubernetes.io/docs/reference/generated/kubernetes-api/v1.20/#containerport-v1-core
  ## e.g:
  ## - name: myapp
  ##   containerPort: 8000
  ##   protocol: TCP
  ##
  extraContainerPorts: []
  ## @param worker.daemonMemoryLimit Set the memory limit for the worker daemon
  ##
  daemonMemoryLimit: ""
  ## @param worker.memoryLimit Set the maximum memory the worker is allowed to use
  ##
  memoryLimit: ""
  ## @param worker.coreLimit Se the maximum number of cores that the worker can use
  ##
  coreLimit: ""
  ## @param worker.dir Set a custom working directory for the application
  ##
  dir: ""
  ## @param worker.javaOptions Set options for the JVM in the form `-Dx=y`
  ##
  javaOptions: ""
  ## @param worker.configOptions Set extra options to configure the worker in the form `-Dx=y`
  ##
  configOptions: ""
  ## @param worker.extraEnvVars An array to add extra env vars
  ## For example:
  ## extraEnvVars:
  ##  - name: SPARK_DAEMON_JAVA_OPTS
  ##    value: -Dx=y
  ##
  extraEnvVars: []
  ## @param worker.extraEnvVarsCM Name of existing ConfigMap containing extra env vars for worker nodes
  ##
  extraEnvVarsCM: "cm-env"
  ## @param worker.extraEnvVarsSecret Name of existing Secret containing extra env vars for worker nodes
  ##
  extraEnvVarsSecret: ""
  ## @param worker.replicaCount Number of spark workers (will be the minimum number when autoscaling is enabled)
  ##
  replicaCount: 2
  ## Kubernetes Pods Security Context
  ## https://kubernetes.io/docs/tasks/configure-pod-container/security-context/
  ## @param worker.podSecurityContext.enabled Enable security context
  ## @param worker.podSecurityContext.fsGroupChangePolicy Set filesystem group change policy
  ## @param worker.podSecurityContext.sysctls Set kernel settings using the sysctl interface
  ## @param worker.podSecurityContext.supplementalGroups Set filesystem extra groups
  ## @param worker.podSecurityContext.fsGroup Group ID for the container
  ## @param worker.podSecurityContext.seLinuxOptions [object,nullable] SELinux options for the container
  ##
  podSecurityContext:
    enabled: true
    fsGroupChangePolicy: Always
    sysctls: []
    supplementalGroups: []
    fsGroup: 1001
    seLinuxOptions: {}
  ## Configure Container Security Context
  ## ref: https://kubernetes.io/docs/tasks/configure-pod-container/security-context/#set-the-security-context-for-a-container
  ## @param worker.containerSecurityContext.enabled Enabled containers' Security Context
  ## @param worker.containerSecurityContext.seLinuxOptions [object,nullable] Set SELinux options in container
  ## @param worker.containerSecurityContext.runAsUser Set containers' Security Context runAsUser
  ## @param worker.containerSecurityContext.runAsGroup Set containers' Security Context runAsGroup
  ## @param worker.containerSecurityContext.runAsNonRoot Set container's Security Context runAsNonRoot
  ## @param worker.containerSecurityContext.privileged Set container's Security Context privileged
  ## @param worker.containerSecurityContext.readOnlyRootFilesystem Set container's Security Context readOnlyRootFilesystem
  ## @param worker.containerSecurityContext.allowPrivilegeEscalation Set container's Security Context allowPrivilegeEscalation
  ## @param worker.containerSecurityContext.capabilities.drop List of capabilities to be dropped
  ## @param worker.containerSecurityContext.seccompProfile.type Set container's Security Context seccomp profile
  ##
  containerSecurityContext:
    enabled: false
    seLinuxOptions: {}
    runAsUser: 1001
    runAsGroup: 1001
    runAsNonRoot: false
    privileged: true
    readOnlyRootFilesystem: false
    allowPrivilegeEscalation: true
    capabilities:
      drop: ["ALL"]
    seccompProfile:
      type: "RuntimeDefault"
  ## @param worker.command Override default container command (useful when using custom images)
  ##
  command: []
  ## @param worker.args Override default container args (useful when using custom images)
  ##
  args: []
  ## @param worker.podAnnotations Annotations for pods in StatefulSet
  ##
  podAnnotations: {}
  ## @param worker.podLabels Extra labels for pods in StatefulSet
  ##
  podLabels: {}
  ## @param worker.podAffinityPreset Spark worker pod affinity preset. Ignored if `worker.affinity` is set. Allowed values: `soft` or `hard`
  ## ref: https://kubernetes.io/docs/concepts/scheduling-eviction/assign-pod-node/#inter-pod-affinity-and-anti-affinity
  ##
  podAffinityPreset: ""
  ## @param worker.podAntiAffinityPreset Spark worker pod anti-affinity preset. Ignored if `worker.affinity` is set. Allowed values: `soft` or `hard`
  ## ref: https://kubernetes.io/docs/concepts/scheduling-eviction/assign-pod-node/#inter-pod-affinity-and-anti-affinity
  ##
  podAntiAffinityPreset: soft
  ## Spark worker node affinity preset
  ## ref: https://kubernetes.io/docs/concepts/scheduling-eviction/assign-pod-node/#node-affinity
  ##
  nodeAffinityPreset:
    ## @param worker.nodeAffinityPreset.type Spark worker node affinity preset type. Ignored if `worker.affinity` is set. Allowed values: `soft` or `hard`
    ##
    type: ""
    ## @param worker.nodeAffinityPreset.key Spark worker node label key to match Ignored if `worker.affinity` is set.
    ## E.g.
    ## key: "kubernetes.io/e2e-az-name"
    ##
    key: ""
    ## @param worker.nodeAffinityPreset.values Spark worker node label values to match. Ignored if `worker.affinity` is set.
    ## E.g.
    ## values:
    ##   - e2e-az1
    ##   - e2e-az2
    ##
    values: []
  ## @param worker.affinity Spark worker affinity for pod assignment
  ## ref: https://kubernetes.io/docs/concepts/configuration/assign-pod-node/#affinity-and-anti-affinity
  ## Note: worker.podAffinityPreset, worker.podAntiAffinityPreset, and worker.nodeAffinityPreset will be ignored when it's set
  ##
  affinity: {}
  ## @param worker.nodeSelector Spark worker node labels for pod assignment
  ## ref: https://kubernetes.io/docs/concepts/scheduling-eviction/assign-pod-node/
  ##
  nodeSelector: {}
  ## @param worker.tolerations Spark worker tolerations for pod assignment
  ## ref: https://kubernetes.io/docs/concepts/configuration/taint-and-toleration/
  ##
  tolerations: []
  ## @param worker.updateStrategy.type Worker statefulset strategy type.
  ## ref: https://kubernetes.io/docs/concepts/workloads/controllers/deployment/#strategy
  ## e.g:
  ## updateStrategy:
  ##  type: RollingUpdate
  ##  rollingUpdate:
  ##    maxSurge: 25%
  ##    maxUnavailable: 25%
  ##
  updateStrategy:
    type: RollingUpdate
  ## @param worker.podManagementPolicy Statefulset Pod Management Policy Type
  ## ref: https://kubernetes.io/docs/concepts/workloads/controllers/statefulset/#pod-management-policies
  ##
  podManagementPolicy: OrderedReady
  ## @param worker.priorityClassName worker pods' priorityClassName
  ##
  priorityClassName: ""
  ## @param worker.topologySpreadConstraints Topology Spread Constraints for pod assignment spread across your cluster among failure-domains. Evaluated as a template
  ## Ref: https://kubernetes.io/docs/concepts/workloads/pods/pod-topology-spread-constraints/#spread-constraints-for-pods
  ##
  topologySpreadConstraints: []
  ## @param worker.schedulerName Name of the k8s scheduler (other than default) for worker pods
  ## ref: https://kubernetes.io/docs/tasks/administer-cluster/configure-multiple-schedulers/
  ##
  schedulerName: ""
  ## @param worker.terminationGracePeriodSeconds Seconds Redmine pod needs to terminate gracefully
  ## ref: https://kubernetes.io/docs/concepts/workloads/pods/pod/#termination-of-pods
  ##
  terminationGracePeriodSeconds: ""
  ## @param worker.lifecycleHooks for the worker container(s) to automate configuration before or after startup
  ##
  lifecycleHooks: {}
  ## @param worker.extraVolumes Optionally specify extra list of additional volumes for the worker pod(s)
  ##
  extraVolumes:
  - name: install-deps
    configMap:
      name: cm-install-deps
  - name: jars
    emptyDir: {}
  - name: start-scripts
    configMap:
      name: cm-start-scripts
  - name: spark-apps
    hostPath:
      path: /data/spark/master/apps
  - name: spark-extra-jars
    hostPath:
      path: /data/spark/master/extra-jars
  ## @param worker.extraVolumeMounts Optionally specify extra list of additional volumeMounts for the master container(s)
  ##
  extraVolumeMounts:
  - name: jars
    mountPath: /jars
  - name: start-scripts
    mountPath: /scripts
  - name: spark-apps
    mountPath: /data/spark/master/apps
  - name: spark-extra-jars
    mountPath: /data/spark/master/extra-jars
  ## @param worker.extraVolumeClaimTemplates Optionally specify extra list of volumesClaimTemplates for the worker statefulset
  ##
  extraVolumeClaimTemplates: []
  ## Container resource requests and limits
  ## ref: https://kubernetes.io/docs/concepts/configuration/manage-compute-resources-container/
  ## We usually recommend not to specify default resources and to leave this as a conscious
  ## choice for the user. This also increases chances charts run on environments with little
  ## resources, such as Minikube. If you do want to specify resources, uncomment the following
  ## lines, adjust them as necessary, and remove the curly braces after 'resources:'.
  ## @param worker.resourcesPreset Set container resources according to one common preset (allowed values: none, nano, micro, small, medium, large, xlarge, 2xlarge). This is ignored if worker.resources is set (worker.resources is recommended for production).
  ## More information: https://github.com/bitnami/charts/blob/main/bitnami/common/templates/_resources.tpl#L15
  ##
  resourcesPreset: "none"
  ## @param worker.resources Set container requests and limits for different resources like CPU or memory (essential for production workloads)
  ## Example:
  ## resources:
  ##   requests:
  ##     cpu: 2
  ##     memory: 512Mi
  ##   limits:
  ##     cpu: 3
  ##     memory: 1024Mi
  ##
  resources:
    requests:
      cpu: 100m
      memory: 512Mi
    limits:
      cpu: 2000m
      memory: 8192Mi
  ## Configure extra options for liveness probe
  ## ref: https://kubernetes.io/docs/tasks/configure-pod-container/configure-liveness-readiness-probes/#configure-probes
  ## @param worker.livenessProbe.enabled Enable livenessProbe
  ## @param worker.livenessProbe.initialDelaySeconds Initial delay seconds for livenessProbe
  ## @param worker.livenessProbe.periodSeconds Period seconds for livenessProbe
  ## @param worker.livenessProbe.timeoutSeconds Timeout seconds for livenessProbe
  ## @param worker.livenessProbe.failureThreshold Failure threshold for livenessProbe
  ## @param worker.livenessProbe.successThreshold Success threshold for livenessProbe
  ##
  livenessProbe:
    enabled: true
    initialDelaySeconds: 180
    periodSeconds: 20
    timeoutSeconds: 5
    failureThreshold: 6
    successThreshold: 1
  ## Configure extra options for readiness probe
  ## ref: https://kubernetes.io/docs/tasks/configure-pod-container/configure-liveness-readiness-probes/#configure-probes
  ## @param worker.readinessProbe.enabled Enable readinessProbe
  ## @param worker.readinessProbe.initialDelaySeconds Initial delay seconds for readinessProbe
  ## @param worker.readinessProbe.periodSeconds Period seconds for readinessProbe
  ## @param worker.readinessProbe.timeoutSeconds Timeout seconds for readinessProbe
  ## @param worker.readinessProbe.failureThreshold Failure threshold for readinessProbe
  ## @param worker.readinessProbe.successThreshold Success threshold for readinessProbe
  ##
  readinessProbe:
    enabled: true
    initialDelaySeconds: 30
    periodSeconds: 10
    timeoutSeconds: 5
    failureThreshold: 6
    successThreshold: 1
  ## Configure extra options for startup probe
  ## @param worker.startupProbe.enabled Enable startupProbe
  ## @param worker.startupProbe.initialDelaySeconds Initial delay seconds for startupProbe
  ## @param worker.startupProbe.periodSeconds Period seconds for startupProbe
  ## @param worker.startupProbe.timeoutSeconds Timeout seconds for startupProbe
  ## @param worker.startupProbe.failureThreshold Failure threshold for startupProbe
  ## @param worker.startupProbe.successThreshold Success threshold for startupProbe
  ##
  startupProbe:
    enabled: true
    initialDelaySeconds: 30
    periodSeconds: 10
    timeoutSeconds: 5
    failureThreshold: 6
    successThreshold: 1
  ## @param worker.customLivenessProbe Custom livenessProbe that overrides the default one
  ##
  customLivenessProbe: {}
  ## @param worker.customReadinessProbe Custom readinessProbe that overrides the default one
  ##
  customReadinessProbe: {}
  ## @param worker.customStartupProbe Custom startupProbe that overrides the default one
  ##
  customStartupProbe: {}
  ## Network Policies
  ## Ref: https://kubernetes.io/docs/concepts/services-networking/network-policies/
  ##
  networkPolicy:
    ## @param worker.networkPolicy.enabled Specifies whether a NetworkPolicy should be created
    ##
    enabled: true
    ## @param worker.networkPolicy.allowExternal Don't require client label for connections
    ## The Policy model to apply. When set to false, only pods with the correct
    ## client label will have network access to the ports the application is listening
    ## on. When true, the app will accept connections from any source
    ## (with the correct destination port).
    ##
    allowExternal: true
    ## @param worker.networkPolicy.allowExternalEgress Allow the pod to access any range of port and all destinations.
    ##
    allowExternalEgress: true
    ## @param worker.networkPolicy.extraIngress [array] Add extra ingress rules to the NetworkPolicy
    ## e.g:
    ## extraIngress:
    ##   - ports:
    ##       - port: 1234
    ##     from:
    ##       - podSelector:
    ##           - matchLabels:
    ##               - role: frontend
    ##       - podSelector:
    ##           - matchExpressions:
    ##               - key: role
    ##                 operator: In
    ##                 values:
    ##                   - frontend
    extraIngress: []
    ## @param worker.networkPolicy.extraEgress [array] Add extra ingress rules to the NetworkPolicy
    ## e.g:
    ## extraEgress:
    ##   - ports:
    ##       - port: 1234
    ##     to:
    ##       - podSelector:
    ##           - matchLabels:
    ##               - role: frontend
    ##       - podSelector:
    ##           - matchExpressions:
    ##               - key: role
    ##                 operator: In
    ##                 values:
    ##                   - frontend
    ##
    extraEgress: []
    ## @param worker.networkPolicy.ingressNSMatchLabels [object] Labels to match to allow traffic from other namespaces
    ## @param worker.networkPolicy.ingressNSPodMatchLabels [object] Pod labels to match to allow traffic from other namespaces
    ##
    ingressNSMatchLabels: {}
    ingressNSPodMatchLabels: {}
  ## @param worker.sidecars Add additional sidecar containers to the worker pod(s)
  ## e.g:
  ## sidecars:
  ##   - name: your-image-name
  ##     image: your-image
  ##     imagePullPolicy: Always
  ##     ports:
  ##       - name: portname
  ##         containerPort: 1234
  ##
  sidecars: []
  ## @param worker.initContainers Add initContainers to the worker pods.
  ## Example:
  ## initContainers:
  ##   - name: your-image-name
  ##     image: your-image
  ##     imagePullPolicy: Always
  ##     ports:
  ##       - name: portname
  ##         containerPort: 1234
  ##
  initContainers:
  - name: download-dependencies
    image: maven
    command:
    - sh
    - -ec
    - |
      cp /scripts/install-deps.sh /opt/install-deps.sh
      chmod +x /opt/install-deps.sh
      /opt/install-deps.sh
    volumeMounts:
    - mountPath: /jars
      name: jars
    - mountPath: /scripts
      name: install-deps
  ## Pod Disruption Budget configuration
  ## ref: https://kubernetes.io/docs/tasks/run-application/configure-pdb
  ## @param worker.pdb.create Enable/disable a Pod Disruption Budget creation
  ## @param worker.pdb.minAvailable Minimum number/percentage of pods that should remain scheduled
  ## @param worker.pdb.maxUnavailable Maximum number/percentage of pods that may be made unavailable. Defaults to `1` if both `worker.pdb.minAvailable` and `worker.pdb.maxUnavailable` are empty.
  ##
  pdb:
    create: true
    minAvailable: ""
    maxUnavailable: ""
  ## Autoscaling parameters
  ## @param worker.autoscaling.enabled Enable replica autoscaling depending on CPU
  ## @param worker.autoscaling.minReplicas Minimum number of worker replicas
  ## @param worker.autoscaling.maxReplicas Maximum number of worker replicas
  ## @param worker.autoscaling.targetCPU Target CPU utilization percentage
  ## @param worker.autoscaling.targetMemory Target Memory utilization percentage
  ##
  autoscaling:
    enabled: false
    minReplicas: ""
    maxReplicas: 5
    targetCPU: 50
    targetMemory: ""
## @section Security parameters
##

## Security configuration
##
security:
  ## @param security.passwordsSecretName Name of the secret that contains all the passwords
  ## This is optional, by default random passwords are generated
  ##
  passwordsSecretName: ""
  ## RPC configuration
  ## @param security.rpc.authenticationEnabled Enable the RPC authentication
  ## @param security.rpc.encryptionEnabled Enable the encryption for RPC
  ##
  rpc:
    authenticationEnabled: false
    encryptionEnabled: false
  ## @param security.storageEncryptionEnabled Enables local storage encryption
  ##
  storageEncryptionEnabled: false
  ## @param security.certificatesSecretName Name of the secret that contains the certificates.
  ## It should contains two keys called "spark-keystore.jks" and "spark-truststore.jks" with the files in JKS format.
  ## DEPRECATED. Use `security.ssl.existingSecret` instead
  ##
  certificatesSecretName: ""
  ## SSL configuration
  ##
  ssl:
    ## @param security.ssl.enabled Enable the SSL configuration
    ##
    enabled: false
    ## @param security.ssl.needClientAuth Enable the client authentication
    ##
    needClientAuth: false
    ## @param security.ssl.protocol Set the SSL protocol
    ##
    protocol: TLSv1.2
    ## @param security.ssl.existingSecret Name of the existing secret containing the TLS certificates
    ## It should contains two keys called "spark-keystore.jks" and "spark-truststore.jks" with the files in JKS format.
    ##
    existingSecret: ""
    ## @param security.ssl.autoGenerated Create self-signed TLS certificates. Currently only supports PEM certificates
    ## The Spark container will generate a JKS keystore and trustore using the PEM certificates.
    ##
    autoGenerated: false
    ## @param security.ssl.keystorePassword Set the password of the JKS Keystore
    ##
    keystorePassword: ""
    ## @param security.ssl.truststorePassword Truststore password.
    ##
    truststorePassword: ""
    ## Container resource requests and limits
    ## ref: https://kubernetes.io/docs/concepts/configuration/manage-compute-resources-container/
    ## We usually recommend not to specify default resources and to leave this as a conscious
    ## choice for the user. This also increases chances charts run on environments with little
    ## resources, such as Minikube. If you do want to specify resources, uncomment the following
    ## lines, adjust them as necessary, and remove the curly braces after 'resources:'.
    ## @param security.ssl.resourcesPreset Set container resources according to one common preset (allowed values: none, nano, micro, small, medium, large, xlarge, 2xlarge). This is ignored if security.ssl.resources is set (security.ssl.resources is recommended for production).
    ## More information: https://github.com/bitnami/charts/blob/main/bitnami/common/templates/_resources.tpl#L15
    ##
    resourcesPreset: "small"
    ## @param security.ssl.resources Set container requests and limits for different resources like CPU or memory (essential for production workloads)
    ## Example:
    ## resources:
    ##   requests:
    ##     cpu: 2
    ##     memory: 512Mi
    ##   limits:
    ##     cpu: 3
    ##     memory: 1024Mi
    ##
    resources: {}
## @section Traffic Exposure parameters
##

## Service parameters
##
service:
  ## @param service.type Kubernetes Service type
  ##
  type: NodePort
  ## @param service.ports.http Spark client port for HTTP
  ## @param service.ports.https Spark client port for HTTPS
  ## @param service.ports.cluster Spark cluster port
  ##
  ports:
    http: 80
    https: 443
    cluster: 7077
  ## Specify the nodePort(s) value(s) for the LoadBalancer and NodePort service types.
  ## ref: https://kubernetes.io/docs/concepts/services-networking/service/#type-nodeport
  ## @param service.nodePorts.http Kubernetes web node port for HTTP
  ## @param service.nodePorts.https Kubernetes web node port for HTTPS
  ## @param service.nodePorts.cluster Kubernetes cluster node port
  ##
  nodePorts:
    http: ""
    https: ""
    cluster: ""
  ## @param service.clusterIP Spark service Cluster IP
  ## e.g.:
  ## clusterIP: None
  ##
  clusterIP: ""
  ## @param service.loadBalancerIP Load balancer IP if spark service type is `LoadBalancer`
  ## Set the LoadBalancer service type to internal only
  ## ref: https://kubernetes.io/docs/concepts/services-networking/service/#internal-load-balancer
  ##
  loadBalancerIP: ""
  ## @param service.loadBalancerSourceRanges Spark service Load Balancer sources
  ## ref: https://kubernetes.io/docs/tasks/access-application-cluster/configure-cloud-provider-firewall/#restrict-access-for-loadbalancer-service
  ## e.g:
  ## loadBalancerSourceRanges:
  ##   - 10.10.10.0/24
  ##
  loadBalancerSourceRanges: []
  ## @param service.externalTrafficPolicy Spark service external traffic policy
  ## ref http://kubernetes.io/docs/tasks/access-application-cluster/create-external-load-balancer/#preserving-the-client-source-ip
  ##
  externalTrafficPolicy: Cluster
  ## @param service.annotations Additional custom annotations for Spark service
  ##
  annotations: {}
  ## @param service.extraPorts Extra ports to expose in Spark service (normally used with the `sidecars` value)
  ##
  extraPorts:
  - name: thrift
    port: 10000
    targetPort: 10000
  ## @param service.sessionAffinity Control where client requests go, to the same pod or round-robin
  ## Values: ClientIP or None
  ## ref: https://kubernetes.io/docs/concepts/services-networking/service/
  ##
  sessionAffinity: None
  ## @param service.sessionAffinityConfig Additional settings for the sessionAffinity
  ## sessionAffinityConfig:
  ##   clientIP:
  ##     timeoutSeconds: 300
  ##
  sessionAffinityConfig: {}
  ## Headless service properties
  ##
  headless:
    ## @param service.headless.annotations Annotations for the headless service.
    ##
    annotations: {}
## Configure the ingress resource that allows you to access the
## Spark installation. Set up the URL
## ref: https://kubernetes.io/docs/concepts/services-networking/ingress/
##
ingress:
  ## @param ingress.enabled Enable ingress controller resource
  ##
  enabled: true
  ## @param ingress.pathType Ingress path type
  ##
  pathType: ImplementationSpecific
  ## @param ingress.apiVersion Force Ingress API version (automatically detected if not set)
  ##
  apiVersion: ""
  ## @param ingress.hostname Default host for the ingress resource
  ##
  hostname: spark.tkg-des.nuvem.unicamp.br
  ## @param ingress.ingressClassName IngressClass that will be be used to implement the Ingress (Kubernetes 1.18+)
  ## This is supported in Kubernetes 1.18+ and required if you have more than one IngressClass marked as the default for your cluster .
  ## ref: https://kubernetes.io/blog/2020/04/02/improvements-to-the-ingress-api-in-kubernetes-1.18/
  ##
  ingressClassName: ""
  ## @param ingress.path The Path to Spark. You may need to set this to '/*' in order to use this with ALB ingress controllers.
  ##
  path: /
  ## @param ingress.annotations Additional annotations for the Ingress resource. To enable certificate autogeneration, place here your cert-manager annotations.
  ## For a full list of possible ingress annotations, please see
  ## ref: https://github.com/kubernetes/ingress-nginx/blob/main/docs/user-guide/nginx-configuration/annotations.md
  ## Use this parameter to set the required annotations for cert-manager, see
  ## ref: https://cert-manager.io/docs/usage/ingress/#supported-annotations
  ##
  ## e.g:
  ## annotations:
  ##   kubernetes.io/ingress.class: nginx
  ##   cert-manager.io/cluster-issuer: cluster-issuer-name
  ##
  annotations:
    cert-manager.io/cluster-issuer: letsencrypt
  ## @param ingress.tls Enable TLS configuration for the hostname defined at ingress.hostname parameter
  ## TLS certificates will be retrieved from a TLS secret with name: {{- printf "%s-tls" .Values.ingress.hostname }}
  ## You can use the ingress.secrets parameter to create this TLS secret or rely on cert-manager to create it
  ##
  tls: true
  ## @param ingress.selfSigned Create a TLS secret for this ingress record using self-signed certificates generated by Helm
  ##
  selfSigned: false
  ## @param ingress.extraHosts The list of additional hostnames to be covered with this ingress record.
  ## Most likely the hostname above will be enough, but in the event more hosts are needed, this is an array
  ## extraHosts:
  ## - name: spark.local
  ##   path: /
  ##
  extraHosts: []
  ## @param ingress.extraPaths Any additional arbitrary paths that may need to be added to the ingress under the main host.
  ## For example: The ALB ingress controller requires a special rule for handling SSL redirection.
  ## extraPaths:
  ## - path: /*
  ##   backend:
  ##     serviceName: ssl-redirect
  ##     servicePort: use-annotation
  ##
  extraPaths: []
  ## @param ingress.extraTls The tls configuration for additional hostnames to be covered with this ingress record.
  ## see: https://kubernetes.io/docs/concepts/services-networking/ingress/#tls
  ## extraTls:
  ## - hosts:
  ##     - spark.local
  ##   secretName: spark.local-tls
  ##
  extraTls: []
  ## @param ingress.secrets If you're providing your own certificates, please use this to add the certificates as secrets
  ## key and certificate should start with -----BEGIN CERTIFICATE----- or
  ## -----BEGIN RSA PRIVATE KEY-----
  ##
  ## name should line up with a tlsSecret set further up
  ## If you're using cert-manager, this is unneeded, as it will create the secret for you if it is not set
  ##
  ## It is also possible to create and manage the certificates outside of this helm chart
  ## Please see README.md for more information
  ## e.g:
  ## - name: spark.local-tls
  ##   key:
  ##   certificate:
  ##
  secrets: []
  ## @param ingress.extraRules Additional rules to be covered with this ingress record
  ## ref: https://kubernetes.io/docs/concepts/services-networking/ingress/#ingress-rules
  ## e.g:
  ## extraRules:
  ## - host: spark.local
  ##     http:
  ##       path: /
  ##       backend:
  ##         service:
  ##           name: spark-svc
  ##           port:
  ##             name: http
  ##
  extraRules: []
## @section Other parameters
##
